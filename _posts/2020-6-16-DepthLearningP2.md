---
layout: post
title: "深度学习-概率与信息论"
date: 2020-6-17 
description: "深度学习，学习笔记"
tag: DepthLearning 
--- 


----



## 概率与信息论


### 1. 个人想法

个人感觉，概率在深度学习中的作用就是通过已知数据去推测未知事件发生的概率，选择一个最有可能的结果作为输出数据。
基于已有数据的形式，概率分布可以分为离散型变量和概率质量函数（不是概率密度函数？）现实中数据的数据绝大多数都是离散型变量，
但在数据分析时往往需要概率质量函数来进行一系列的数学运算。因此，如何从离散的概率数据中分析出其中蕴含的数学模型是所有操作的第一步。

如果某一件事发生的影响因素比较多，就引入了联合概率分布和边缘概率等。通过分析不同因素对结果的影响，确定每个因素的权重。
这里就引入了“高斯混合模型”，高斯混合模型是个万能近似器。在这种意义下，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近。
说虽然这么说呀，但足够多这个前置条件已经限制了很多东西。

### 2. 条件概率及其链式法则

条件概率就是在知道某一件事发生的前提下，另一件事发生的概率。



第一次看懂条件概率|的意思，表示|前面的东西是在|后面发生的条件下发生的概率。以前，|和，搞在一起就有点看不懂。

任何多维随机变量的联合概率分布，都可以分解为只有一个变量的条件概率相乘的形式：
$$P(a,b,c) = P(a|b,c)*P(b,c)$$
$$P(b,c) = P(b|c)*P(c)$$
$$P(a,b,c) = P(a|b,c)P(b|c)*P(c)$$


### 3. 期望、方差和协方差

方差衡量的是当我们对x依据它的概率分布进行采样时，随机变量x的函数值会呈现多大的差异：
$$Var(f(x)) = E[(f(x) - E(f(x)))^2]$$

协方差在某种意义上给出了两个变量线性相关的强度以及这些变量的尺度：
$$Cov(f(x),g(y)) = E[(f(x) - E[f(x)])(g(y) - E[g(y)])]$$

协方差的绝对值如果很大，则意味着变量值变化很大，并且他们同时距离各自的均值很远。如果协方差是正的，那么两个变量都倾向于同时取得相对较大的值。
如果协方差是负的，那么其中一个变量倾向于取得相对较大值的同时，另一个变量倾向于取得相对较小的值，反之亦然。

### 4. 常用函数的有用性质

logistic sigmoid:
$$f(x) = 1/(1+e(-x))$$
logistic sigmoid函数可以将变量平滑的转化为（0，1）之间。sigmoid函数在变量取绝对值非常大的正值或者负值时会出现饱和现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感。

softplus:
$$f(x) = log(1+exp(x))$$
softplus函数是在线性函数到0之间寻求了一个平滑过渡。

### 5. 信息论

信息论的基本想法就是量化一个信息所蕴含的内容。特点是：
> * 非常可能发生的事件信息量比较少，并且极端情况下，确保能够发生的事件应该没有信息量。
> * 较不可能发生的事情具有更高的信息量。
> * 独立事件应具有增量的信息。例如，投掷的硬币两次正面向上传递的信息量，应该是投掷一次硬币正面向上信息量的两倍。

为了满足上述3个性质，我们定义一个事件x=x的自信息为
$$I(x) = -logP(x)$$
注：log表示的自然对数，其底数为e。因此，我们定义的I(x)单位是奈特(nats)。一奈特是以1/e的概率观测到一个事件时获得的信息量。还可以使用底数为2的对数，单位是比特(bit)或者香农；

自信息只处理单个的输出。还可以使用香农熵来对整个概率分布中的不确定性总量进行量化：
$$H(x) = Ex~p[I(x)] = -Ex~p[I(x)]$$

如果对于同一个随机变量x有两个单独的概率分布P(x)和Q(x)，可以使用KL散度（Kullback-Leibler(KL)divergence）来衡量这两个分布的差异：
$$Dkl(P||Q) = Ex~p[log(P(x)/Q(x))] = Ex~p[logP(x) - logQ(x)]$$

